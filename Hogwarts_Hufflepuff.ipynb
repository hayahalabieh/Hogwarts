{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hogwarts School of Data Wizardry\n",
    "## Session 4: Intro to Data Science Methods!\n",
    "`Hufflepuff`  |   [GitHub](https://github.tesla.com/EHSS/Hogwarts) | [Documentation](https://confluence.teslamotors.com/display/EHSSST/Hogwarts+School+of+Data+Wizardry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################################################################################################\n",
    "#################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "### [1. Natural Language Processing (NLP)](#1-natural-language-processing)\n",
    "- [1.1 Text Cleaning](#11-text-cleaning)\n",
    "- [1.2 Making a Dictionary](#12-creating-a-dictionary-ie-list-of-all-words)\n",
    "### [2. Logistic Regression Examples](#2-logistic-regression)\n",
    "- [2.1 Bag of Words](#21-bag-of-words)\n",
    "### [3. k Nearest Neighbors](#3-knn----k-nearest-neighbors)\n",
    "### [4. BERT](#4-bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################################################################################################\n",
    "#################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Text Cleaning\n",
    "\n",
    "Remove contractions, turn all text to lowercase, and create a vocab list (i.e. dictionary) of all words in the text column of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "  \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a dictionary, i.e. list of all words\n",
    "\n",
    "Now we are creating a dictionary of all words submitted to our dataset. The process to do this is by taking the series `text_column`, then iterating through every list within this series, then every word within each list. \n",
    "\n",
    "We then check to see if the word is already in the dictionary , and if it is not, the word is then appended. This process repeats for every row. \n",
    "\n",
    "Once words is initialized, we remove any empty strings and alphabetize it. The dictionary is now ready to be used for future natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_maker(text_column):\n",
    "    dictionary = []\n",
    "    for row in text_column:\n",
    "        for word in row:\n",
    "            if word not in dictionary:\n",
    "                dictionary.append(word)\n",
    "    \n",
    "    #remove empty strings\n",
    "    for word in dictionary:\n",
    "        if word == \"\":\n",
    "            dictionary.remove(word)\n",
    "            \n",
    "    #sort dictionary\n",
    "    dictionary.sort()\n",
    "\n",
    "    print(f\"The dictionary contains {str(len(dictionary))} words\")\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the test size as a decimal (percentage) for the train test split as the `test_size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a train-validation split on our labeled data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#FILL ME IN\n",
    "df = \"\"\n",
    "test_frac = \"\"\n",
    "\n",
    "def train_test_sets(dataframe, test_size):\n",
    "    train, val = train_test_split(dataframe, test_size, random_state = 42)\n",
    "    print(\"The training set is \" + str(len(train))  + \" data points\")\n",
    "    print(\"The validation set is \"  + str(len(val)) +  \" data points\")\n",
    "    return train, val\n",
    "\n",
    "train, val = train_test_sets(dataframe=df, test_size=test_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(text_column, train_set, test_set):\n",
    "    '''\n",
    "    @PARAM\n",
    "    text_column == string of the column in the dataframe where the text for analysis is\n",
    "    train_set == set of divided training data\n",
    "    test_set == set of divided validation data\n",
    "    '''\n",
    "    bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False) \n",
    "    X_tr_bow = bow_transform.fit_transform(train_set[text_column])\n",
    "    X_te_bow = bow_transform.transform(test_set[text_column])\n",
    "\n",
    "    return X_tr_bow, X_te_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. kNN -- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN is an algorithm that uses the norm to find the nearest neighbors.\n",
    "\n",
    "In this case, the \"norm\" is the distance between any 2 data points, often computed as the Euclidean distance:\n",
    "Norm of $ (x_1, y_1) $ and $ (x_2, y_2) =  (y_2 - y_1) / (x_2 - x_1)$\n",
    "\n",
    "### @PARAM\n",
    "\n",
    "k is an odd integer:\n",
    "$ k = 2n + 1 \\ \\ \\ \\ \\  \\forall \\ \\  n \\in N $\n",
    "\n",
    "\n",
    "#### Q: Why must k be an odd integer?\n",
    "\n",
    "--\n",
    "\n",
    "### @RETURN\n",
    "\n",
    "The classification = norm of neighbors. \n",
    "\n",
    "i.e. Imagine k = 9 for a given data point, which we are trying to classify as red or blue. If 5 neighbors are red, 4 neighbors are blue. Mode (most frequently occurring value) is red, thus the item is classified as red. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(dataframe, text_col, class_col, k, test_split_size):\n",
    "    '''\n",
    "    @PARAM\n",
    "    dataframe == the dataframe of relevance\n",
    "    text_col == the name of the column that contains text for analysis\n",
    "    k == the number of neighbors (MUST BE ODD)\n",
    "    test_split_size == fraction of data split for testing set (float)\n",
    "    '''\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    text_encoded = encoder.fit_transform(dataframe.text_col)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_encoded.reshape(-1,1),dataframe.class_col,test_size=test_split_size)\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Predicted',y_pred)\n",
    "    print('Actual data',y_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT\n",
    "### Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "\n",
    "BERT was developped by Google for predicting words in sentences. The algorithm has a fairly high accuracy rate. It is not difficult to use once it is understood. \n",
    "\n",
    "def `tranfsormer`: the attention mechanism that learns contextual relationships between words in a text.\n",
    "\n",
    "-- \n",
    "\n",
    "#### Training Stratgies. \n",
    "1. Masked LM\n",
    "    - adds a classification layer on top of the encoder output\n",
    "    - process includes multiplying the output layer by an embedding matrix\n",
    "    - words are transformed to the vocab dimension\n",
    "2. Next Sentence Prediction\n",
    "    - labels 2 given sentences as `IsNext` or `NotNext`\n",
    "    - 50% of sentences are a pair of subsequent sentences\n",
    "    - 50% of sentenes are a random corpus sentence pair\n",
    "\n",
    "--\n",
    "\n",
    "#### DistilBERT\n",
    "DistilBERT is knowledge distillation of the BERT algorithm, known as a lite.\n",
    "\n",
    "Essentially it is a compression technique from a large to smaller model with similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "from tensorflow import keras\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert(text_col, classification_col, test_set_size, classification_names):\n",
    "  '''\n",
    "  @PARAM\n",
    "  assuming\n",
    "  text_col == the dataframe column of text (strings)\n",
    "  classification_col == the dataframe column of (numeric) classifications, otherwise change .to_numpy() to .tolist()\n",
    "  test_set_size == how large the test set should be\n",
    "  classification_names == list of classifications (i.e. ['red', 'blue'])\n",
    "\n",
    "  @RETURN\n",
    "  #TODO: FILL IN RETURN VAL\n",
    "  '''\n",
    "  x_train, x_val, y_train, y_val = train_test_split(text_col.tolist(), classification_col.to_numpy(), \n",
    "                                              shuffle=True, test_size = test_set_size, random_state=12342, stratify=classification_col)\n",
    "                      \n",
    "\n",
    "  (x_train_bert, y_train_bert), (x_val_bert, y_val_bert), preproc = text.texts_from_array(x_train=x_train, y_train=y_train, x_test=x_val, y_test=y_val, \n",
    "                                                                                    class_names=[\"0\", \"1\"], \n",
    "                                                                                      preprocess_mode=\"bert\", \n",
    "                                                                                      lang = \"en\", \n",
    "                                                                                      maxlen=65, \n",
    "                                                                                      max_features=35000)\n",
    "                                          \n",
    "\n",
    "  model = text.text_classifier('bert', train_data=(x_train_bert, y_train_bert), preproc=preproc)\n",
    "  learner = ktrain.get_learner(model, train_data=(x_train_bert, y_train_bert), val_data=(x_val_bert, y_val_bert), batch_size=16)\n",
    "\n",
    "  #finding the best learning rate via plotting\n",
    "  learner.lr_find(), learner.lr_plot()\n",
    "  min = \"...\" #TODO: Automate finding the min\n",
    "  learner.autofit(min)\n",
    "\n",
    "  #validate the data\n",
    "  learner.validate(val_data=(x_val_bert, y_val_bert), class_names=classification_names)\n",
    "\n",
    "  predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "  learner.print_layers()\n",
    "\n",
    "  #TODO\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62eaf0967e57fc27937e4fba2e5ff8a8c48118aaa21290068a641422be1f8935"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
